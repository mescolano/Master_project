{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2  Calculating the aggregate demand with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore our full dataset with a Spark mindset. We have already explored it in the introduction with pandas to know what it contains and how it looks like but now we need to prepare it to be able to do the calculations and aggregations we need with the appropriate data types. We will start with the data sample and then try the scheme with the full dataset. As a result, we will build a program that can be run in HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample\n",
    "!wget 'https://files.datapress.com/london/dataset/smartmeter-energy-use-data-in-london-households/UKPN-LCL-smartmeter-sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Full dataset\n",
    "!wget 'https://files.datapress.com/london/dataset/smartmeter-energy-use-data-in-london-households/Power-Networks-LCL-June2015(withAcornGps).zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File transformation to bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is compressed in .zip format. The problem of this format is it cannot be broken into pieces without being corrupted. Therefore, if we attempt a MapReduce operation we will get a useless binary output. To be able to use it without decompressing it (as we do not have enough disk space) we will transform it line by line to bzip2 using pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!unzip -c 'Power-Networks-LCL-June2015(withAcornGps).zip'  | bzip2 > 'Power-Networks-LCL-June2015.bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start. The commented lines would be needed if our Virtual Machine had not been already configured to make our life easier to import SparkContext and assign it to sc upon start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f0868237d90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pyspark import Row\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discover the steps needed to process our dataset using the small sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_sample = 'UKPN-LCL-smartmeter-sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample = sc.textFile(path_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'LCLid,stdorToU,DateTime,KWH/hh (per half hour) ,Acorn,Acorn_grouped',\n",
       " u'MAC003718,Std,17/10/2012 13:00:00,0.09,ACORN-A,Affluent',\n",
       " u'MAC003718,Std,17/10/2012 13:30:00,0.16,ACORN-A,Affluent']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header is useful in pandas but we need to remove it when working with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = data_sample.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample2 = data_sample.filter(lambda l: l != header).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'MAC003718,Std,17/10/2012 13:00:00,0.09,ACORN-A,Affluent',\n",
       " u'MAC003718,Std,17/10/2012 13:30:00,0.16,ACORN-A,Affluent',\n",
       " u'MAC003718,Std,17/10/2012 14:00:00,0.212,ACORN-A,Affluent']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will map the file extracting the relevant fields and converting them to the appropriate types. We will ignore the last field, which is actually a higher-level ACORN grouping compared to the second last. ACORN is a UK consumer socioeconomical segmentation system that may be useful in this work (see http://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line2tuple(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        DateTime = datetime.strptime(fields[2], '%d/%m/%Y %H:%M:%S')\n",
    "        ACORN = fields[4]\n",
    "        try:\n",
    "            consumption = float(fields[3])\n",
    "        except ValueError:\n",
    "            consumption = np.nan\n",
    "        return (ID, tariff, DateTime, consumption, ACORN)\n",
    "    else:\n",
    "        return (np.nan,np.nan,np.nan,np.nan,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAC003718',\n",
       "  u'Std',\n",
       "  datetime.datetime(2012, 10, 17, 13, 0),\n",
       "  0.09,\n",
       "  u'ACORN-A'),\n",
       " (u'MAC003718',\n",
       "  u'Std',\n",
       "  datetime.datetime(2012, 10, 17, 13, 30),\n",
       "  0.16,\n",
       "  u'ACORN-A'),\n",
       " (u'MAC003718',\n",
       "  u'Std',\n",
       "  datetime.datetime(2012, 10, 17, 14, 0),\n",
       "  0.212,\n",
       "  u'ACORN-A')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample2.map(line2tuple).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows_sample = data_sample2.map(line2tuple)\\\n",
    ".map(lambda x: Row(ID = x[0], Tariff = x[1], DateTime = x[2], kWh_30min = x[3], ACORN = x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sample = rows_sample.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+------+---------+\n",
      "|  ACORN|            DateTime|       ID|Tariff|kWh_30min|\n",
      "+-------+--------------------+---------+------+---------+\n",
      "|ACORN-A|2012-10-17 13:00:...|MAC003718|   Std|     0.09|\n",
      "|ACORN-A|2012-10-17 13:30:...|MAC003718|   Std|     0.16|\n",
      "|ACORN-A|2012-10-17 14:00:...|MAC003718|   Std|    0.212|\n",
      "|ACORN-A|2012-10-17 14:30:...|MAC003718|   Std|    0.145|\n",
      "|ACORN-A|2012-10-17 15:00:...|MAC003718|   Std|    0.104|\n",
      "+-------+--------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try calculating the mean grouping by tariff. In this case, there is only one consumer, which is subject to the standard tariff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sample.dropna().groupBy('Tariff').mean().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[ACORN: string, DateTime: timestamp, ID: string, Tariff: string, kWh_30min: double]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the types, everything looks correct.\n",
    "df_sample.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving the data to a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of our full dataset processing, we will aggregate the data and and save it to a text file. Let us explore alternatives with the smaller sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different options to save the data as a text file. The problem with these first two options is that it will save it in several text files, as it does not coalesce the Spark DataFrame in one partition before writing the text file. This is the only option when the resulting DataFrame is too large to save it in one local partition but is not so convenient for future processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.18 ms, sys: 155 µs, total: 2.33 ms\n",
      "Wall time: 1.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sample.write.csv('sample.csv') # Needs Spark >= 2.0. In the cluster, we have Spark 1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 ms, sys: 176 µs, total: 1.49 ms\n",
      "Wall time: 1.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sample.write.format('com.databricks.spark.csv').save('sample2.csv') # For Spark 1.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to save the data in one file, we can use these handier alternatives. Our first aggregations of the full dataset will be small enough (~ 80k lines, ~500k lines) to be able to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.57 ms, sys: 1.22 ms, total: 6.79 ms\n",
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sample.rdd.map(lambda x: ','.join(map(str, x))).coalesce(1).saveAsTextFile('file_rdd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 446 ms, sys: 59.1 ms, total: 505 ms\n",
      "Wall time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sample.toPandas().to_csv('file_pd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming it to a Pandas DataFrame was quicker, simpler and the exit file includes the header. We will follow this path when data is small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start tackling the real thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Power-Networks-LCL-June2015.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Archive:  Power-Networks-LCL-June2015(withAcornGps).zip',\n",
       " u'  inflating: Power-Networks-LCL-June2015(withAcornGps)v2.csv  ',\n",
       " u'LCLid,stdorToU,DateTime,KWH/hh (per half hour) ,Acorn,Acorn_grouped',\n",
       " u'MAC000002,Std,2012-10-12 00:30:00.0000000, 0 ,ACORN-A,Affluent',\n",
       " u'MAC000002,Std,2012-10-12 01:00:00.0000000, 0 ,ACORN-A,Affluent']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two new problems.\n",
    "\n",
    "First of all, there are two extra lines above the header, so now we need to remove 3 lines before mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_no_header = data.zipWithIndex().filter(lambda x: x[1] > 2).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAC000002', u'Std', u'2012-10-12 00:30:00.0000000', 0.0, u'ACORN-A'),\n",
       " (u'MAC000002', u'Std', u'2012-10-12 01:00:00.0000000', 0.0, u'ACORN-A'),\n",
       " (u'MAC000002', u'Std', u'2012-10-12 01:30:00.0000000', 0.0, u'ACORN-A')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_header.map(line2tuple).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the DateTime format has changed compared to the sample provided!! Not very nice, let's hope that at least it is consistent thoughout the whole full dataset.\n",
    "We must change our mapping function accordingly. This datetime format can be automatically changed later for the full dataset to pyspark.sql TimestampType so we will leave it as string for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAC000005', u'ToU', u'2012-06-01 10:30:00.0000000', 0.095, u'ACORN-C'),\n",
       " (u'MAC000005', u'ToU', u'2012-06-01 11:00:00.0000000', 0.051, u'ACORN-C'),\n",
       " (u'MAC000005', u'ToU', u'2012-06-01 11:30:00.0000000', 0.098, u'ACORN-C')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_header.map(line2tuple).filter(lambda (_1,t,_2,_3,_4): t == 'ToU').take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAC000005', u'ToU', u'2012-06-01 10:30:00.0000000', 0.095, u'ACORN-C'),\n",
       " (u'MAC000005', u'ToU', u'2012-06-01 11:00:00.0000000', 0.051, u'ACORN-C'),\n",
       " (u'MAC000005', u'ToU', u'2012-06-01 11:30:00.0000000', 0.098, u'ACORN-C')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_header.map(line2tuple)\\\n",
    ".filter(lambda (u,t,_2,_3,_4): (u == 'MAC000005') & (t=='ToU')).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So actually \"stdorToU\" tells us if a user was subjected to dToU or std tariff in 2013, not in the timestamp corresponding to a given measurement. We do not need to separate users then but we should bear in mind later that dToU tariff was only applied in 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line2tuple(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6: \n",
    "\n",
    "#We have seen that there is at least one line in the file that does not follow the format.\n",
    "#This condition is to avoid an \"Index out of range\" error. \n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        DateTime = fields[2]\n",
    "        ACORN = fields[4]\n",
    "        try:\n",
    "            consumption = float(fields[3])\n",
    "        except ValueError:\n",
    "            consumption = np.nan\n",
    "        return (ID, tariff, DateTime, consumption,ACORN)\n",
    "    else:\n",
    "        return (np.nan,np.nan,np.nan,np.nan,np.nan)\n",
    "   \n",
    "#We treat any line that does not follow the general format as NaN. \n",
    "#We will remove them afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our dataframe and check that DateTime is a string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = data_no_header.map(line2tuple)\\\n",
    ".map(lambda x: Row(ID = x[0], Tariff = x[1], DateTime = x[2], kWh_30min = x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DateTime: string, ID: string, Tariff: string, kWh_30min: double]>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change DateTime to pyspark.sql TimestampType (this is equivalent to datetime.datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "df2 = df.withColumn('DateTime', df['DateTime'].cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DateTime: timestamp, ID: string, Tariff: string, kWh_30min: double]>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+---------+\n",
      "|            DateTime|       ID|Tariff|kWh_30min|\n",
      "+--------------------+---------+------+---------+\n",
      "|2012-10-12 00:30:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 01:00:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 01:30:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 02:00:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 02:30:...|MAC000002|   Std|      0.0|\n",
      "+--------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try if calculating the mean grouped by tariff works, this would mean we have cleaned the dataset successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Tariff=u'Std', avg(kWh_30min)=0.21507225601128732),\n",
       " Row(Tariff=u'ToU', avg(kWh_30min)=0.1986226410448441)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dropna().groupBy('Tariff').mean().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! We have also learnt the keys used to name the two tariff groups we expected to find ('Std' and 'ToU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful in our analysis which consumers were subjected to Dynamic Time of Use (ToU) tariff during 2013. These had a standard tariff during the rest of the period included in the dataset so we can evaluate behavioural changes due to the tariff scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.filter(df2['Tariff'] == 'ToU').select('ID').drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step of our analysis we will aggregate all users under the same tariff plan (distinguishing consumers subjected to ToU during 2013 and consumers with the standard flat rate tariff throughout the whole period of time). We will analyse this data in R later (no need for Spark), including explanatory plots. R provides useful packages for this purpose: xts and zoo for time series plotting and forecast for prediction models.\n",
    "\n",
    "Therefore, we will calculate basic statistics for each timestamp per tariff plan. We will then refine the analysis aggregating by ACORN group and tariff plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ACORN(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        acorn = fields[4]\n",
    "        return acorn\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ACORN groups\n",
    "ACORN_groups = data_no_header.map(ACORN).filter(lambda a: a != 'NA').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ACORN-K',\n",
       " u'ACORN-J',\n",
       " u'ACORN-I',\n",
       " u'ACORN-',\n",
       " u'ACORN-H',\n",
       " u'ACORN-O',\n",
       " u'ACORN-N',\n",
       " u'ACORN-M',\n",
       " u'ACORN-L',\n",
       " u'ACORN-C',\n",
       " u'ACORN-B',\n",
       " u'ACORN-Q',\n",
       " u'ACORN-A',\n",
       " u'ACORN-P',\n",
       " u'ACORN-G',\n",
       " u'ACORN-F',\n",
       " u'ACORN-E',\n",
       " u'ACORN-U',\n",
       " u'ACORN-D']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACORN_groups.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for script to be executed in the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pyspark import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Power-Networks-LCL-June2015.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(path)\n",
    "data_no_header = data.zipWithIndex().filter(lambda x: x[1] > 2).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ToU_IDs(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        return (tariff, ID)\n",
    "    else:\n",
    "        return ('NA','NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID_ToU_rdd = data_no_header.map(ToU_IDs).filter(lambda (t,_): t == 'ToU').distinct()\n",
    "ID_ToU = ID_ToU_rdd.map(lambda (_,i): i).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line2tuple(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        DateTime = fields[2]\n",
    "        ACORN = fields[4]\n",
    "        try:\n",
    "            consumption = float(fields[3])\n",
    "        except ValueError:\n",
    "            consumption = np.nan\n",
    "        return (ID, tariff, DateTime, consumption, ACORN)\n",
    "    else:\n",
    "        return (np.nan,np.nan,np.nan,np.nan,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = data_no_header.map(line2tuple)\\\n",
    ".map(lambda x: Row(ID = x[0], Tariff = x[1], DateTime = x[2], \n",
    "                   kWh_30min = x[3], ACORN = x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+------+--------+---------+\n",
      "|  ACORN|            DateTime|       ID|Tariff|ToU_User|kWh_30min|\n",
      "+-------+--------------------+---------+------+--------+---------+\n",
      "|ACORN-A|2012-10-12 00:30:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 01:00:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 01:30:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 02:00:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 02:30:...|MAC000002|   Std|       0|      0.0|\n",
      "+-------+--------------------+---------+------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "df = df.withColumn('DateTime', df['DateTime'].cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[ACORN: string, DateTime: timestamp, ID: string, Tariff: string, ToU_User: bigint, kWh_30min: double]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_stats = df.dropna().groupBy('DateTime','Tariff')\\\n",
    "    .agg(F.count('kWh_30min').alias('count'),\n",
    "         F.sum('kWh_30min').alias('sum'),\n",
    "         F.min('kWh_30min').alias('min'),\n",
    "         F.mean('kWh_30min').alias('mean'),\n",
    "         F.max('kWh_30min').alias('max'),\n",
    "         F.stddev('kWh_30min').alias('std_dev')\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the grouped Dataframe is small enough to be saved in one partition we can transform it to a Pandas dataframe and then save it to csv. Saving it in only one text file is much handier for future processing.\n",
    "\n",
    "We could have also transformed it back to a functional API rdd and used coalesce(1).saveAsTextFile('agg_stats.csv').\n",
    "\n",
    "Otherwise, we could have written it to multiple text files with:\n",
    "\n",
    "agg_stats.write.csv('agg_stats.csv') (Spark >= 2.0)\n",
    "\n",
    "agg_stats.write.format('com.databricks.spark.csv').save('agg_stats.csv') (in the cluster, where we have Spark 1.5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_stats.toPandas().to_csv('outputs/agg_stats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acorn_stats = df.dropna().groupBy('ACORN','DateTime','Tariff')\\\n",
    "    .agg(F.count('kWh_30min').alias('count'),\n",
    "         F.sum('kWh_30min').alias('sum'),\n",
    "         F.min('kWh_30min').alias('min'),\n",
    "         F.mean('kWh_30min').alias('mean'),\n",
    "         F.max('kWh_30min').alias('max'),\n",
    "         F.stddev('kWh_30min').alias('std_dev')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acorn_stats.toPandas().to_csv('outputs/acorn_stats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will copy this last part to SmartMeter_agg1.py to run it in the cluster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
