{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will explore our full dataset with a Spark mindset. We have already explored it in the introduction with pandas to know what it contains and how it looks like but now we need to prepare it to be able to do the calculations and aggregations we need with the appropriate data types. We will start with the data sample and then try the scheme with the full dataset. As a result, we will build a program that can be run in the HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to download the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sample\n",
    "!wget 'https://files.datapress.com/london/dataset/smartmeter-energy-use-data-in-london-households/UKPN-LCL-smartmeter-sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Full dataset\n",
    "!wget 'https://files.datapress.com/london/dataset/smartmeter-energy-use-data-in-london-households/Power-Networks-LCL-June2015(withAcornGps).zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is compressed in .zip format. The problem of this format is it cannot be broken into pieces without being corrupted. Therefore, if we attempt a MapReduce operation we will get a useless binary output. To be able to use it without decompressing it (as we do not have enough disk space) we will transform it line by line to bzip2 using pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!unzip -c 'Power-Networks-LCL-June2015(withAcornGps).zip'  | bzip2 > 'Power-Networks-LCL-June2015(withAcornGps).bz2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to start. The commented lines would be needed if our Virtual Machine had not been already configured to make our life easier to import SparkContext and assign it to sc upon start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7fc54575bd90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pyspark import Row\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'2.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_sample = 'UKPN-LCL-smartmeter-sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample = sc.textFile(path_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'LCLid,stdorToU,DateTime,KWH/hh (per half hour) ,Acorn,Acorn_grouped',\n",
       " u'MAC003718,Std,17/10/2012 13:00:00,0.09,ACORN-A,Affluent',\n",
       " u'MAC003718,Std,17/10/2012 13:30:00,0.16,ACORN-A,Affluent']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header is useful in pandas but we need to remove it when working with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "header = data_sample.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sample2 = data_sample.filter(lambda l: l != header).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'MAC003718,Std,17/10/2012 13:00:00,0.09,ACORN-A,Affluent',\n",
       " u'MAC003718,Std,17/10/2012 13:30:00,0.16,ACORN-A,Affluent',\n",
       " u'MAC003718,Std,17/10/2012 14:00:00,0.212,ACORN-A,Affluent']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample2.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will map the file extracting the relevant fields and converting them to the appropriate types. We will ignore the last field, which is actually a higher-level ACORN grouping compared to the second last. ACORN is a UK consumer socioeconomical segmentation system that may be useful in this work (see http://acorn.caci.co.uk/downloads/Acorn-User-guide.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line2tuple(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        DateTime = datetime.strptime(fields[2], '%d/%m/%Y %H:%M:%S')\n",
    "        ACORN = fields[4]\n",
    "        try:\n",
    "            consumption = float(fields[3])\n",
    "        except ValueError:\n",
    "            consumption = np.nan\n",
    "        return (ID, tariff, DateTime, consumption, ACORN)\n",
    "    else:\n",
    "        return (np.nan,np.nan,np.nan,np.nan,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAC003718',\n",
       "  u'Std',\n",
       "  datetime.datetime(2012, 10, 17, 13, 0),\n",
       "  0.09,\n",
       "  u'ACORN-A'),\n",
       " (u'MAC003718',\n",
       "  u'Std',\n",
       "  datetime.datetime(2012, 10, 17, 13, 30),\n",
       "  0.16,\n",
       "  u'ACORN-A'),\n",
       " (u'MAC003718',\n",
       "  u'Std',\n",
       "  datetime.datetime(2012, 10, 17, 14, 0),\n",
       "  0.212,\n",
       "  u'ACORN-A')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample2.map(line2tuple).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = data_sample2.map(line2tuple)\\\n",
    ".map(lambda x: Row(ID = x[0], Tariff = x[1], DateTime = x[2], kWh_30min = x[3], ACORN = x[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+------+---------+\n",
      "|  ACORN|            DateTime|       ID|Tariff|kWh_30min|\n",
      "+-------+--------------------+---------+------+---------+\n",
      "|ACORN-A|2012-10-17 13:00:...|MAC003718|   Std|     0.09|\n",
      "|ACORN-A|2012-10-17 13:30:...|MAC003718|   Std|     0.16|\n",
      "|ACORN-A|2012-10-17 14:00:...|MAC003718|   Std|    0.212|\n",
      "|ACORN-A|2012-10-17 14:30:...|MAC003718|   Std|    0.145|\n",
      "|ACORN-A|2012-10-17 15:00:...|MAC003718|   Std|    0.104|\n",
      "+-------+--------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try calculating the mean grouping by tariff. In this case, there is only one consumer, which is subject to the standard tariff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Tariff=u'Std', avg(kWh_30min)=0.20900675947184585)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna().groupBy('Tariff').mean().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DateTime: timestamp, ID: string, Tariff: string, kWh_30min: double]>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the types, everything looks correct.\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start tackling the real thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Power-Networks-LCL-June2015(withAcornGps).bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Archive:  Power-Networks-LCL-June2015(withAcornGps).zip',\n",
       " u'  inflating: Power-Networks-LCL-June2015(withAcornGps)v2.csv  ',\n",
       " u'LCLid,stdorToU,DateTime,KWH/hh (per half hour) ,Acorn,Acorn_grouped',\n",
       " u'MAC000002,Std,2012-10-12 00:30:00.0000000, 0 ,ACORN-A,Affluent',\n",
       " u'MAC000002,Std,2012-10-12 01:00:00.0000000, 0 ,ACORN-A,Affluent']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two new problems.\n",
    "\n",
    "First of all, there are two extra lines above the header, so now we need to remove 3 lines before mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_no_header = data.zipWithIndex().filter(lambda x: x[1] > 2).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MAC000002', u'Std', u'2012-10-12 00:30:00.0000000', 0.0),\n",
       " (u'MAC000002', u'Std', u'2012-10-12 01:00:00.0000000', 0.0),\n",
       " (u'MAC000002', u'Std', u'2012-10-12 01:30:00.0000000', 0.0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_header.map(line2tuple).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the DateTime format has changed compared to the sample provided!! Not very nice, let's hope that at least it is consistent thoughout the whole full dataset.\n",
    "We must change our mapping function accordingly. This datetime format can be automatically changed later for the full dataset to pyspark.sql TimestampType so we will leave it as string for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line2tuple(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6: \n",
    "\"\"\"\n",
    "We have seen that there is at least one line in the file that does not follow the format.\n",
    "This condition is to avoid an \"Index out of range\" error. \n",
    "\"\"\"\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        DateTime = fields[2]\n",
    "        ACORN = fields[4]\n",
    "        try:\n",
    "            consumption = float(fields[3])\n",
    "        except ValueError:\n",
    "            consumption = np.nan\n",
    "        return (ID, tariff, DateTime, consumption,ACORN)\n",
    "    else:\n",
    "        return (np.nan,np.nan,np.nan,np.nan,np.nan)\n",
    "\"\"\"    \n",
    "    We treat any line that does not follow the general format as NaN. \n",
    "    We will remove them afterwards\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our dataframe and check that DateTime is a string variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = data_no_header.map(line2tuple)\\\n",
    ".map(lambda x: Row(ID = x[0], Tariff = x[1], DateTime = x[2], kWh_30min = x[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DateTime: string, ID: string, Tariff: string, kWh_30min: double]>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We change DateTime to pyspark.sql TimestampType (this is equivalent to datetime.datetime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "df2 = df.withColumn('DateTime', df['DateTime'].cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[DateTime: timestamp, ID: string, Tariff: string, kWh_30min: double]>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------+---------+\n",
      "|            DateTime|       ID|Tariff|kWh_30min|\n",
      "+--------------------+---------+------+---------+\n",
      "|2012-10-12 00:30:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 01:00:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 01:30:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 02:00:...|MAC000002|   Std|      0.0|\n",
      "|2012-10-12 02:30:...|MAC000002|   Std|      0.0|\n",
      "+--------------------+---------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try if calculating the mean grouped by tariff works, this would mean we have cleaned the dataset successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Tariff=u'Std', avg(kWh_30min)=0.21507225601128732),\n",
       " Row(Tariff=u'ToU', avg(kWh_30min)=0.1986226410448441)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dropna().groupBy('Tariff').mean().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's great! We have also learnt the keys used to name the two tariff groups we expected to find ('Std' and 'ToU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful in our analysis which consumers were subjected to Dynamic Time of Use (ToU) tariff during 2013. These had a standard tariff during the rest of the period included in the dataset so we can evaluate behavioural changes due to the tariff scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.filter(df2['Tariff'] == 'ToU').select('ID').drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a variable to our dataframe indicating whether a user is always subject to the standard flat rate or is subject to the dToU tariff during 2013. In order to be more efficient this should be done before mapping the text file.\n",
    "\n",
    "We can now write our script to parse the full text file dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step of our analysis we will aggregate all users under the same tariff plan (distinguishing consumers subjected to ToU during 2013 and consumers with the standard flat rate tariff throughout the whole period of time). We can analyze this data in Python later (no need for Spark), including explanatory plots.\n",
    "\n",
    "Therefore, we will calculate basic statistics for each timestamp per tariff plan. We will then refine the analysis aggregating by ACORN group and tariff plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ToU_IDs(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        return (tariff, ID)\n",
    "    else:\n",
    "        return ('NA','NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID_ToU_rdd = data_no_header.map(ToU_IDs).filter(lambda (t,_): t == 'ToU').distinct()\n",
    "ID_ToU = ID_ToU_rdd.map(lambda (_,i): i).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ACORN(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        acorn = fields[4]\n",
    "        return acorn\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ACORN groups\n",
    "ACORN_groups = data_no_header.map(ACORN).filter(lambda a: a != 'NA').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ACORN-K',\n",
       " u'ACORN-J',\n",
       " u'ACORN-I',\n",
       " u'ACORN-',\n",
       " u'ACORN-H',\n",
       " u'ACORN-O',\n",
       " u'ACORN-N',\n",
       " u'ACORN-M',\n",
       " u'ACORN-L',\n",
       " u'ACORN-C',\n",
       " u'ACORN-B',\n",
       " u'ACORN-Q',\n",
       " u'ACORN-A',\n",
       " u'ACORN-P',\n",
       " u'ACORN-G',\n",
       " u'ACORN-F',\n",
       " u'ACORN-E',\n",
       " u'ACORN-U',\n",
       " u'ACORN-D']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ACORN_groups.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for script to be executed in the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext\n",
    "#sc = SparkContext()\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pyspark import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Power-Networks-LCL-June2015(withAcornGps).bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = sc.textFile(path)\n",
    "data_no_header = data.zipWithIndex().filter(lambda x: x[1] > 2).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ToU_IDs(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        return (tariff, ID)\n",
    "    else:\n",
    "        return ('NA','NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ID_ToU_rdd = data_no_header.map(ToU_IDs).filter(lambda (t,_): t == 'ToU').distinct()\n",
    "ID_ToU = ID_ToU_rdd.map(lambda (_,i): i).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line2tuple(l):\n",
    "    fields = l.split(',')\n",
    "    if len(fields) == 6:\n",
    "        ID = fields[0]\n",
    "        tariff = fields[1]\n",
    "        DateTime = fields[2]\n",
    "        ACORN = fields[4]\n",
    "        ToU_User = 1 if ID in ID_ToU else 0 \n",
    "        try:\n",
    "            consumption = float(fields[3])\n",
    "        except ValueError:\n",
    "            consumption = np.nan\n",
    "        return (ID, tariff, DateTime, consumption, ACORN, ToU_User)\n",
    "    else:\n",
    "        return (np.nan,np.nan,np.nan,np.nan,np.nan,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = data_no_header.map(line2tuple)\\\n",
    ".map(lambda x: Row(ID = x[0], Tariff = x[1], DateTime = x[2], \n",
    "                   kWh_30min = x[3], ACORN = x[4], ToU_User = x[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rows.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------+------+--------+---------+\n",
      "|  ACORN|            DateTime|       ID|Tariff|ToU_User|kWh_30min|\n",
      "+-------+--------------------+---------+------+--------+---------+\n",
      "|ACORN-A|2012-10-12 00:30:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 01:00:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 01:30:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 02:00:...|MAC000002|   Std|       0|      0.0|\n",
      "|ACORN-A|2012-10-12 02:30:...|MAC000002|   Std|       0|      0.0|\n",
      "+-------+--------------------+---------+------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType\n",
    "df = df.withColumn('DateTime', df['DateTime'].cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[ACORN: string, DateTime: timestamp, ID: string, Tariff: string, ToU_User: bigint, kWh_30min: double]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agg_stats = df.dropna().groupBy('DateTime','Tariff', 'ToU_User')\\\n",
    "    .agg(F.count('kWh_30min').alias('count'),\n",
    "         F.sum('kWh_30min').alias('sum'),\n",
    "         F.min('kWh_30min').alias('min'),\n",
    "         F.mean('kWh_30min').alias('mean'),\n",
    "         F.max('kWh_30min').alias('max'),\n",
    "         F.stddev('kWh_30min').alias('std_dev')\n",
    "        ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DateTime=datetime.datetime(2012, 10, 15, 21, 0), Tariff=u'Std', ToU_User=0, count=4210, sum=1270.5869999, min=0.0, mean=0.3018021377434679, max=5.335, std_dev=0.3191298313348088),\n",
       " Row(DateTime=datetime.datetime(2012, 10, 21, 21, 0), Tariff=u'Std', ToU_User=0, count=4287, sum=1239.8139993, min=0.0, mean=0.28920317221833447, max=6.0949998, std_dev=0.32085161430326753),\n",
       " Row(DateTime=datetime.datetime(2012, 10, 27, 13, 0), Tariff=u'Std', ToU_User=0, count=4402, sum=1159.6950000000002, min=0.0, mean=0.2634472966833258, max=3.5539999, std_dev=0.3363976758973744),\n",
       " Row(DateTime=datetime.datetime(2012, 10, 28, 17, 30), Tariff=u'Std', ToU_User=0, count=4402, sum=1665.0080014, min=0.0, mean=0.3782389825988187, max=8.04, std_dev=0.44731524288964336),\n",
       " Row(DateTime=datetime.datetime(2012, 11, 4, 0, 30), Tariff=u'Std', ToU_User=0, count=4403, sum=994.769, min=0.0, mean=0.2259298205768794, max=6.072, std_dev=0.42469688066123107)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_stats[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('agg_stats.txt', 'w') as f:\n",
    "    for i in agg_stats:\n",
    "        f.write(\"{}\\n\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acorn_stats = df.dropna().groupBy('ACORN','DateTime','Tariff', 'ToU_User')\\\n",
    "    .agg(F.count('kWh_30min').alias('count'),\n",
    "         F.sum('kWh_30min').alias('sum'),\n",
    "         F.min('kWh_30min').alias('min'),\n",
    "         F.mean('kWh_30min').alias('mean'),\n",
    "         F.max('kWh_30min').alias('max'),\n",
    "         F.stddev('kWh_30min').alias('std_dev')\n",
    "        ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('acorn_stats.txt', 'w') as f:\n",
    "    for i in acorn_stats:\n",
    "        f.write(\"{}\\n\".format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
